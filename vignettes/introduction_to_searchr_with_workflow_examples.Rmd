---
title: "Introduction to litsearchr with an example of writing a systematic review search strategy"
author: "Eliza Grames and Andrew Stillman"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to litsearchr with an example of writing a systematic review search strategy}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


# About litsearchr

## Introduction

The **litsearchr** package for R is designed to partially automate search term selection and writing search strategies for systematic reviews. It uses the Rapid Automatic Keyword Extraction algorithm [@Rose2010] to identify potential keywords from a sample of titles and abstracts and combines them with author- and database-tagged keywords to create a pool of possible keywords relevant to a field of study. Important keywords in a field are identified from their importance in a keyword co-occurrence network. After keywords are grouped into concept groups, **litsearchr** writes Boolean searches in up to 53 languages, with stemming support for English. The searches work fully in 14 commonly used search databases with partial support in six additional databases.

Our intent in creating **litsearchr** is to make the process of designing a search strategy for systematic reviews easier for researchers in fields that lack Medical Subject Headings (MeSH) by processing the terms commonly used in a field. By partially automating keyword selection, **litsearchr** reduces investigator bias in keyword selection and increases the repeatability of systematic reviews. It also reduces errors in creating database-specific searches by generating searches that work across a wide range of databases. My hope is that **litsearchr** can be used to facilitate systematic reviews and contribute to automating evidence synthesis. 

## Acknowledgements

I want to thank:
 - My co-author Andrew Stillman for using his study system as an example, providing feedback and suggestions at every step of the project, and testing the precision and recall of the package
 - Chris Elphick and Morgan Tingley for feedback, support, and advice at every stage of this project
 - Carolyn Mills for advice on which databases to search, testing database search rules, and guiding us through the process of manually writing a search strategy
 - Robi Bagchi for the initial idea to use co-occurrence networks for keyword identification
 - Theresa Wisneskie for testing early versions of the package
 - Developers of the packages on which **litsearchr** relies: dplyr, freeknotsplines, gdata, igraph, knitr, quanteda, rapidraker, rmarkdown, SnowballC, splines2, stringr, tm, and wordcloud.

## Comments, suggestions, bugs, and questions

**litsearchr** is a work in progress - any and all comments, suggestions, bugs, or questions are welcome! Please email eliza.grames\@uconn.edu or open an issue at  https://github.com/elizagrames/litsearchr.

# Installation and dependencies

To install **litsearchr** use devtools::install_github("elizagrames/litsearchr"). 

# Workflow example: Preparing a search strategy for a systematic review

There are lots of reasons to do a systematic literature review; for example, this package could be to identify research gaps in a field of study or prepare a search strategy as part of a meta-analysis. In this example, we're developing a complete search strategy for a systematic review of papers that could potentially answer the question: What **processes** lead to immediate Black-backed **Woodpecker** **occupancy** of **post-fire** forest systems and subsequent decline with time since fire? Black-backed woodpeckers are known to colonize post-fire areas soon after fire, and their numbers gradually decrease as the number of years since fire increases [@Tingley2018]. We would like to conduct a systematic literature review that draws on literature from similar systems to identify potential mechanisms that might produce this pattern.

We start with a scoping search, which is a search of a few databases (namely, Scopus, BIOSIS, and Zoological Record) using a simple Boolean search. We will then use litsearchr to identify additional relevant keywords that are used in the literature but aren't included in our scoping search. Finally, we will write new Boolean searches using the newly identified search terms to conduct a systematic review in multiple languages. 

## Step 1: Conduct and import the scoping search

We have four main parts to our question: processes, woodpeckers, occupancy, and fire. These four words represent concept groups that must be present in an article for it to be relevant; thus, our scoping search should return articles that include all four concepts. Because we know something about Black-backed Woodpecker (BBWO) ecology, we know that principles of population ecology are likely to be important in determining their occupancy of post-fire systems and should include those in the 'process' concept group. The other concept groups are the most relevant synonyms for those concepts that we already know appear in the literature. The scoping search we came up with is shown below and was conducted in three databases: BIOSIS, Zoological Record, and Web of Science.

Date searches were conducted: 6/21/18. 
Results: 258
(from BIOSIS Citation Index)
You searched for: TOPIC: (( ("nest success" OR birth OR death OR mortalit\* OR immigrat\* OR emigrat\* OR mechanis\* OR caus\*) AND (fire\* OR burn\*) AND (occup\* OR occur\*) AND (woodpecker OR bird OR sapsucker) ))
Timespan: All years. Indexes: BCI.

Results: 177
(from Zoological Record)
You searched for: TOPIC: (( ("nest success" OR birth OR death OR mortalit\* OR immigrat\* OR emigrat\* OR mechanis\* OR caus\*) AND (fire\* OR burn\*) AND (occup\* OR occur\*) AND (woodpecker OR bird OR sapsucker) ))
Timespan: All years. Indexes: Zoological Record.

Results: 203
(from Scopus)
You searched for: TITLE-ABS-KEY ( ( ( "nest success"  OR  birth  OR  death  OR  mortalit\*  OR  immigrat\*  OR  emigrat\*  OR  mechanis\*  OR  caus\* )  AND  ( fire\*  OR  burn\* )  AND  ( occup\*  OR  occur\* )  AND  ( woodpecker  OR  bird  OR  sapsucker ) ) ) 


Although other databases could also be used, the import functions of this package are set up to work with four databases that are commonly used in ecology: BIOSIS, Scopus, Zoological Record, and ProQuest (not included above). Search results should be exported as .csv files from Scopus, tab-delimited (.txt) from BIOSIS and Zoological Record, and as .xls from ProQuest. For BIOSIS, Zoological Record, and ProQuest, you should export the full record; in Scopus the citation information and abstract and keywords should be exported. 

The original export files should not be altered at all - none of the columns need to be removed and default headers should be left alone. These are used as signatures to detect which database a file originated from. If one of your naive searches results in more than 500 hits and you need to export multiple files from BIOSIS or Zoological Record, they can be left as separate files and don't need to be manually combined - searchr will do this for you. However, note that if your naive search returns more than 500 hits, the search terms are likely too broad. This lack of specificity may mean that the updated search terms returned by searchr will not adequately capture the desired level of inference. 

Optionally, if you want to return extremely specific keywords, you can conduct a critical appraisal of your naive search results to remove articles that you know aren't relevant to your question. However, if these articles are relatively rare, their keywords should be filtered out by litsearchr as unimportant. 

All results of scoping searches should be placed into a single directory. The results of the Black-backed Woodpecker example searches are included in the package in ./inst/extdata/ [Note: you may need to change the directory reference depending on your current working directory.]

```{r}
list.files("../inst/extdata/")

```

Now that we have scoping search results, we need to import these to our working environment with the function import_scope(). This function will identify any .txt, .csv, or .xls files in a dataset and import them accordingly. If importing .xls files, the gdata package is required. After loading in files, import_scope() identifies the database that produced the downloaded file (currently, only BIOSIS, Zoological Record, Scopus, and ProQuest are supported), and standardizes their columns. 

If you have scoping search results from a different database, format them (we suggest using the select() function in dplyr) to have the following columns in order: id, text, title, abstract, keywords, methods, type, authors, affiliation, source, year, volume, issue, startpage, endpage, doi, language, database. Columns can be left blank, but all the columns need to be included and ordered correctly so that you can then rbind() your data frame to the data frame created with import_scope(). If you have suggestions for additional databases to support in the import_scope() function, please email eliza.grames\@uconn.edu 

If remove_duplicates is set to TRUE, duplicate articles that share the same journal title, volume number, and start page are removed. This is advised because using multiple databases for the scoping search is likely to result in duplicates because the supported databases overlap in which journals and years they index. Removing duplicates avoids double-counting keywords from the same article which would inflate their importance in the field. 

If clean_dataset is set to TRUE, keyword punctuation is cleaned up and standardized to be separated with a semicolon. 

All the files from the directory are bound to a single dataset which records the database they originated from.


```{r}
BBWO_data <- import_scope("../inst/extdata/", remove_duplicates = TRUE, clean_dataset = TRUE)

```


## Step 2: Identify all potential keywords

The next step is to use this body of text to identify and extract all potential keywords that we may not have included when selecting words for our naive search. There are two approaches to this in searchr: 1) considering just author/database-tagged keywords already attached to articles and 2) scraping the titles and/or abstracts of all articles to identify new keywords. In this example, we'll use both approaches and combine them. 

First, we use the Rapid Automatic Keyword Extraction algorithm [@Rose2010] in the package rapidraker to scrape potential keywords from abstracts and titles. The default options are used, which means (1) we aren't adding new stopwords to ignore, (2) a term must occur at least twice in the entire dataframe to be considered (this eliminates weird phrases that just happen to have no stopwords), and (3) we want to scrape both titles and abstracts.

The amount of time this function takes depends on the size of your database; for ~500 articles it is under a minute. 

```{r}
raked_keywords <- extract_terms(BBWO_data, new_stopwords = NULL, min_freq = 2, title = TRUE, abstract = TRUE)

# When we check the results, some of these are definitely not important keywords
# For example, "cocaine exposure cause parrot foot necrosis" doesn't seem particularly relevant to our woodpecker question
# But, not to worry, these sorts of keywords will disappear once we check their frequency in the entire dataset later
head(raked_keywords, 20)
```

Next, we want to extract the author/database-tagged keywords from our scoping search.

```{r}
real_keywords <- select_actual_terms(BBWO_data)

# At first glance, these keywords seem more relevant
# But something tagged with "biology" really isn't any better than "cocaine exposure cause parrot foot necrosis"
# This is why a combined approach is best to really capture the full range of possible keywords
head(real_keywords, 20)

```

In order to create a document-term matrix that only includes our potential keywords, we need to make a dictionary object to pass to the quanteda package. We also need to create a corpus object for it to read. The corpus and dictionary are passed to create_dfm, which calls the quanteda function dfm(). This creates a document-term matrix (quanteda calls it a document-feature matrix) that contains each document as a row and each term as a column with a count of the number of times the term is used in the corresponding spot. This matrix is going to be the basis for determining which keywords are central to the field. 

```{r}
BBWO_dict <- make_dictionary(actual_terms = real_keywords, likely_terms = raked_keywords)
BBWO_corpus <- make_corpus(BBWO_data)

BBWO_dfm <- create_dfm(BBWO_corpus, my_dic=BBWO_dict)

# If you want to see the most frequently occurring terms, call the topfeatures() function from the quanteda package. 
quanteda::topfeatures(BBWO_dfm, 20)

```

## Step 3: Create keyword co-occurrence network and select nodes

Before we can select important keywords, we need to create a keyword co-occurrence network. A network is stronger than a matrix for node selection because we can examine things like node centrality, degrees, and strength rather than just frequency or other properties we could get from the matrix. 

We pass our BBWO_dfm matrix to create_network() and specify that a keyword must occur at least three times in the entire dataset and in a minimum of three studies - these can be adjusted according to how stringent you want to be. Optionally at this point, you can look at the structure of the complete original network. Depending on the size of your graph and your computing power, this can take some time. 

```{r}
BBWO_graph <- create_network(BBWO_dfm, min_studies=3, min_occurrences = 3)
# plot_full_network(BBWO_graph)
```


Since we have a weighted network, node importance is judged based on node whichever measure of node importance you select: strength, eigencentrality, alpha centrality, betweenness, hub score, or power centrality. For a description of what each of these measures, please refer to the igraph package documentation. The default measure of importance in litsearchr is node strength because it makes intuitive sense for keywords. Stronger nodes are more important in the field and co-occur more often with other strong nodes. Weak nodes occur in the field less frequently and can be disregarded as good search terms. Essentially, identifying strong nodes is a way of generating search terms similar to Medical Subject Headings (MeSH) by looking at what is currently used in the field rather than having pre-determined headings. 

### Using the full network to identify important nodes

There are two ways to identify important nodes in the litsearchr package: fitting a spline model to the node importance to select tipping points or finding the minimum number of nodes to capture a large percent (e.g. 80%) of the total importance of the network. Which way you decide to identify important nodes depends both on how the node importance of your graph are distributed and your personal preference. In this example, we will consider both.

The first thing to do is look at the distribution of node importance (in this case, node strength). From the density and histogram plots, it looks like this network has a lot of fairly weak nodes with a long tail. We're interested in the long tail. There appears to be a pretty sharp cutoff, so a spline model is an appropriate method to identify the cutoff threshold for keyword importance. The cumulative approach is more appropriate when there aren't clear breaks in the data.

When using a spline model, we need to specify where to place knots (i.e. where should the fitted model parameters be allowed to change). Looking at the plot of ranked node importance, we should use a second-degree polynomial for the spline and should probably allow four knots: one around rank 2200 where the curve starts to accelerate, one around rank 2600 where it gets a bit steeper, one around rank 2800 where the steepness changes more drastically, and one around rank 3100 where the curve starts to asymptote. 

```{r}
plot(density(igraph::strength(BBWO_graph)), main="Density of node strengths")
hist(igraph::strength(BBWO_graph), 100, main="Histogram of node strengths", xlab="Node strength")
plot(sort(igraph::strength(BBWO_graph)), ylab="Node strength", main="Ranked node strengths", xlab="Rank")
```


First, we run the spline method and save those cutoff points. Each knot is a potential cutoff point, though in this case we plan to use the lowest knot to capture more potential terms and be less conservative. For more complex node strength curves a different knot might be appropriate. To print the residuals from the spline model fit and the model fit itself, set diagnostics to TRUE. The residuals should resemble a funnel plot centered on zero; large residuals with an increase in both rank and strength are expected. 

For the cumulative method, the only decision is what percent of the total network node strength you want to capture. In this case, we left it at the default of 80%. Diagnostics for the cumulative curve are not actually diagnostic and simply show the cutoff point in terms of rank and strength. 

```{r}
cutoffs_spline <- find_cutoff(BBWO_graph, method = "spline", degrees = 2, knot_num = 4, diagnostics = TRUE)

cutoffs_cumulative <- find_cutoff(BBWO_graph, method = "cumulative", cum_pct = .8, diagnostics = TRUE, importance_method = "strength")
```


We guessed the knot points incorrectly, but both the spline and cumulative fits seem pretty good. The spline fit minimum cutoff is 1383, which is more conservative than the cumulative fit of 970. For this example, we're opting for the spline cutoff and will pass this cutoff strength to the reduce_graph() function which creates a new igraph object that only contains nodes from the original network that surpass a minimum strength threshold of 1383. From the reduced graph, we use get_keywords() to save the keywords to a plain text file and also generate a wordcloud. 

```{r}
reduced_graph <- reduce_graph(BBWO_graph, cutoff_strength = cutoffs_spline[1], printplot = FALSE)

search_terms <- get_keywords(reduced_graph, savekeywords = TRUE, makewordle = FALSE)

# If we check the length of search_terms, it is a pretty long list to consider manually (930)
length(search_terms)

# How relevant are these terms? Not very
head(sort(search_terms), 20)

# We have two options about how to procede here: use a more stringent cutoff for node strength (for example cutoffs_spline[2]), or split out network into n-grams which is the next example

```

### Splitting the full network into subgraphs by n-grams to identify important keywords

Unigrams (keyword phrases with only one word) will tend to be the dominant nodes in a network. For example, "species" will always be more important than "species occurrence" or "species distribution" because both of the 2-grams will be counted in the unigram. To account for this, the package includes a function to split the full network into a graph of n-grams (and if desired, a separate graph of unigrams). This graph can be passed to find_cutoffs() to get the most relevant potential keywords. Essentially, splitting the graph into n-grams will allow you to consider terms that are likely more relevant to your search that are being drowned out by unigrams in the full graph. If you want to bring the unigrams back in, you can always combine them into a joint list. The code for this is shown below, but we won't consider the unigrams as potential keywords. 

```{r}
ngram_graph <- make_ngram_graph(BBWO_graph, min_ngrams = 2)
plot(sort(igraph::strength(ngram_graph)), ylab="Node strength", main="Ranked node strengths", xlab="Rank")

ngram_cutoff_strength <- find_cutoff(ngram_graph, method="spline", degrees = 2, knot_num = 3)
ngram_reduced_graph <- reduce_graph(ngram_graph, cutoff_strength = ngram_cutoff_strength[1])

# Note: to get a wordcloud, you need to install the package wordcloud
ngram_keywords <- get_keywords(ngram_reduced_graph, makewordle=TRUE)

# as expected, the n-gram keywords are much more relevant than the full network
head(ngram_keywords, 20)

unigram_graph <- make_ngram_graph(BBWO_graph, unigrams = TRUE)
plot(sort(igraph::strength(unigram_graph)), ylab="Node strength", main="Ranked node strengths", xlab="Rank")

unigram_cutoff_strength <- find_cutoff(unigram_graph, method="spline", degrees = 2, knot_num = 3)
unigram_reduced_graph <- reduce_graph(unigram_graph, cutoff_strength = unigram_cutoff_strength[2])

unigram_keywords <- get_keywords(unigram_reduced_graph, makewordle = FALSE)


```

## Step 4: Group terms into concepts

We're still only interested in search terms that fall under one of our concept categories: processes, woodpecker, occupancy, and fire. From the list of potential search terms returned, we need to manually go through and select which ones belong in a concept category and add it as a character vector. Any of the original search terms not included in the keywords from the network (e.g. woodpecker) should also be added back in. 

We think it is easiest to sort potential keywords in a .csv file outside of R, so we wrote the search terms to a .csv to go through and assign them to process groups or disregard them. We've done this by adding a column to the .csv for group and assigning terms to either "x" (for disregard) or one of the groups. After scanning the list of keywords, we also decided to add a fifth concept group for habitat.

At this stage, you can also add in additional terms that you've thought of that weren't in your scoping search and weren't found by litsearchr. litsearchr is merely meant as a tool to identify additional keywords - not to restrict your search. 

```{r}
write.csv(ngram_keywords, "~/ngram-potential-keywords.csv")

grouped_keywords <- read.csv("~/ngram-potential-keywords-grouped.csv", 
                             stringsAsFactors = FALSE)

# We append our original search terms for a category with the newly identified terms
process_group <- append(
  c("nest success", "birth", "death", "mortality", 
    "immigration", "emigration", "mechanism", "cause"),
  grouped_keywords$term[which(grouped_keywords$group=="process")])

# An example of adding a keyword - colleagues suggested that "stand-replacing fire" be added
fire_group <- append(
  c("fire","burn", "stand-replacing fire"),
  grouped_keywords$term[which(grouped_keywords$group=="fire")])

# For the bird group, we removed our scoping term "bird" because it isn't specific to woodpeckers
bird_group <- append(
  c("woodpecker", "sapsucker"),
  grouped_keywords$term[which(grouped_keywords$group=="bird")])

occupancy_group <- append(
  c("occupancy", "occupance", "occurrence"),
  grouped_keywords$term[which(grouped_keywords$group=="occupancy")])

# The habitat group is new based on scanning keywords
habitat_group <- grouped_keywords$term[which(grouped_keywords$group=="habitat")]

my_search_terms <- list(process_group, fire_group, bird_group, occupancy_group, habitat_group)

```

## Step 5: Write Boolean searches

litsearchr provides functions to write simple Boolean searches that work in 14 tested databases.

Full support:
 - BIOSIS Citation Index
 - Zoological Record
 - Scopus
 - CAB Direct
 - WorldCat
 - ProQuest Dissertations and Theses
 - OAIster
 - Science.gov
 - Academic Search Premier
 - Agricola
 - CAB Abstracts
 - GeoRef
 - GreenFILE
 - Science Reference Center
 
We did not test all EBSCO databases, but it seems likely that the search string works in all of them as all databases we did test it worked. If you plan to use a different EBSCO database, check the database rules first. If they do not specify whether truncation is allowed within exact phrases, try checking the results of two phrases that should be equivalent (e.g. ["nest predator" OR "nest predators"] and ["nest predator*"]). If the truncated version returns an equal (or sometimes greater) number of articles, truncation within exact phrases is allowed. 

Partial support:
 - Open Access Theses and Dissertations [no stemming within quotes/exact phrases]
 - NDLTD Global ETD Search [no stemming within quotes/exact phrases]
 - Google Scholar [restricted to 256 characters]
 - Helda [no stemming within quotes]
 - Ingenta Connect [inconsistently allows stemming within quotes]
 - JSTOR [no stemming in quotes, limited length, not recommended]
 - Wiley Online Library [no stemming within quotes]


To write foreign language searches, specify the list of your concept groups, the key topics for journal languages, and whether or not terms should be included in quotes to find exact phrases. The translation is done with the Google Translate API using the translate package. You will need to set up an API key to use this function (https://cloud.google.com/translate/). 

To visually see which languages are used in the discipline, we can use the language_graphs() function. If you want to specify the languages to write the search in instead of using the choose_languages() function, simply replace it with a character vector of languages. Available languages can be viewed with available_languages(). 

To write English language searches, set languages to "English". Writing stemmed searches is currently only supported in English.

```{r}
translate_API <- readLines("~/Google-Translate-API")

my_key_topics <- c("biology", "conservation", "ecology", "forestry")

# Depending on how many languages you choose to search in and how many search terms you have, this can take some time
# As each language is written, the console will print "Russian is written!" etc...
# "All done!" will print when all languages are written
write_search(groupdata = my_search_terms, 
             languages = choose_languages(lang_data=get_language_data(key_topics = my_key_topics)[1:10,]), 
             exactphrase = FALSE, directory="~/BBWO-searches/")

language_graphs(lang_data = get_language_data(key_topics = my_key_topics), no_return = 15, key_topics = my_key_topics)

write_stemmed_search(groupdata = my_search_terms, exactphrase = TRUE, directory="~/BBWO-searches/")

```


Now we can inspect our written searches. Foreign language searches will be in unicode, but if you open them in a web browser, you can copy and paste them into search databases. Depending on what fonts you have on your computer, you may or may not be able to get the CJK (Chinese-Japanese-Korean character sets) searches working; my computer will not return CJK characters.

```{r}
readLines("~/BBWO-searches/search-in-Russian.txt")
readLines("~/BBWO-searches/search-in-French.txt")
readLines("~/BBWO-searches/search-in-stemmed-English.txt")

```


```{r}
rm(list=ls())
setwd("~/Documents/research-projects/auto-search/EEJournal/Bernes2017/")
list.files("./")
library(dplyr)
library(litsearchr)

Bernes_include <- gdata::read.xls("13750_2017_103_MOESM3_ESM.xls", header=TRUE, sheet=3, stringsAsFactors = FALSE)
Bernes_found <- read.csv("./final-deduped-Bernes.csv", header=TRUE, stringsAsFactors = FALSE)[1:14]

relevant_studies <- Bernes_include %>%
  select(title=Title, authors=Authors)
relevant_studies$source <- rep("relevant_studies", nrow(relevant_studies))

retrieved_studies <- Bernes_found %>%
  select(title=title, authors=authors)
retrieved_studies$source <- rep("retrieved_studies", nrow(retrieved_studies))

search_metrics <- check_search_strategy(relevant_studies, retrieved_studies)

```

